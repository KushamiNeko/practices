{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import dropout\n",
    "from tensorflow.contrib.layers import conv2d\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path to dataset \n",
    "DATA_ROOT = r\"/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge\"\n",
    "assert(os.path.exists(DATA_ROOT))\n",
    "\n",
    "TRAIN_IMAGES = os.path.join(DATA_ROOT, \"train\")\n",
    "assert(os.path.exists(TRAIN_IMAGES))\n",
    "\n",
    "TRAIN_MASKS = os.path.join(DATA_ROOT, \"train_masks\")\n",
    "assert(os.path.exists(TRAIN_MASKS))\n",
    "\n",
    "TRAIN_MASKS_CSV = os.path.join(DATA_ROOT, \"train_masks.csv\")\n",
    "assert(os.path.exists(TRAIN_MASKS_CSV))\n",
    "\n",
    "METADATA_CSV = os.path.join(DATA_ROOT, \"metadata.csv\")\n",
    "assert(os.path.exists(METADATA_CSV))\n",
    "\n",
    "TEST_IMAGES = os.path.join(DATA_ROOT, \"test\")\n",
    "assert(os.path.exists(TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train/4e7bc95552ed_06.jpg'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train/65051cfe0789_04.jpg'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train/784ca55262c2_06.jpg'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train/4f0397cf7937_02.jpg'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train/8d5423cb763c_05.jpg']\n",
      "[ b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train_masks/4e7bc95552ed_06_mask.gif'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train_masks/65051cfe0789_04_mask.gif'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train_masks/784ca55262c2_06_mask.gif'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train_masks/4f0397cf7937_02_mask.gif'\n",
      " b'/run/media/onionhuang/HDD/ARTIFICIAL_INTELLIGENCE/KAGGLE_COMPETITIONS/Carvana_Image_Masking_Challenge/train_masks/8d5423cb763c_05_mask.gif']\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "def _labels_files(filename, labels_dir):\n",
    "    separator = \"/\"\n",
    "    basename = tf.string_split([filename], separator).values[-1]\n",
    "\n",
    "    name = tf.string_split([basename], \".\").values[0]\n",
    "\n",
    "    new_name = tf.string_join([name, \"mask.gif\"], \"_\")\n",
    "\n",
    "    lables_file = tf.string_join([labels_dir, new_name], separator)\n",
    "\n",
    "    return lables_file\n",
    "\n",
    "\n",
    "files = tf.train.match_filenames_once([TRAIN_IMAGES+\"/*\"] * 2)\n",
    "\n",
    "# files = tf.random_shuffle(files)\n",
    "\n",
    "# print(TRAIN_IMAGES+\"/*\")\n",
    "# \n",
    "labels = tf.map_fn(lambda x: _labels_files(x, TRAIN_MASKS), files)\n",
    "# labels = tf.string_split(files, \"/\")\n",
    "\n",
    "# test = tf.string_split([\"test test\"], \" \")\n",
    "# test_values = test.values\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "tf.local_variables_initializer().run()\n",
    "\n",
    "# print(test_values.eval())\n",
    "\n",
    "print(files.eval()[:5])\n",
    "# print(len(files.eval()))\n",
    "\n",
    "print(labels.eval()[:5])\n",
    "\n",
    "features_dataset = files.eval()\n",
    "labels_dataset = labels.eval()\n",
    "\n",
    "for index, file in enumerate(features_dataset):\n",
    "    basename = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    assert(basename in labels_dataset[index])\n",
    "# print(len(labels.eval()).value)\n",
    "print(\"finish\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# tf.app.flags.DEFINE_integer(\"batch_size\", 10, \"\"\"batch size\"\"\")\n",
    "\n",
    "# path to dataset\n",
    "# DATA_ROOT = os.path.join(\"/run/media/onionhuang/HDD/artificial_intelligence\",\n",
    "#                          \"kaggle/Carvana-Image-Masking-Challenge\")\n",
    "\n",
    "# assert (os.path.exists(DATA_ROOT))\n",
    "\n",
    "# tf.app.flags.DEFINE_string(\"TRAIN_IMAGES\",\n",
    "#                            os.path.join(DATA_ROOT, \"train\"),\n",
    "#                            \"\"\"path to training images\"\"\")\n",
    "\n",
    "# tf.app.flags.DEFINE_string(\"TRAIN_MASKS\",\n",
    "#                            os.path.join(DATA_ROOT, \"train_masks\"),\n",
    "#                            \"\"\"path to training images masks\"\"\")\n",
    "\n",
    "# tf.app.flags.DEFINE_string(\"TRAIN_MASKS_CSV\",\n",
    "#                            os.path.join(DATA_ROOT, \"train_masks.csv\"),\n",
    "#                            \"\"\"path to training images masks in csv format\"\"\")\n",
    "\n",
    "# tf.app.flags.DEFINE_string(\"METADATA_CSV\",\n",
    "#                            os.path.join(DATA_ROOT, \"metadata.csv\"),\n",
    "#                            \"\"\"path to images metadata in csv format\"\"\")\n",
    "\n",
    "# tf.app.flags.DEFINE_string(\"TEST_IMAGES\",\n",
    "#                            os.path.join(DATA_ROOT, \"test\"),\n",
    "#                            \"\"\"path to test images\"\"\")\n",
    "\n",
    "SOURCE_IMAGE_HEIGHT = 1280\n",
    "\n",
    "SOURCE_IMAGE_WIDTH = 1918\n",
    "\n",
    "TARGET_IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_inputs():\n",
    "    if not TRAIN_IMAGES:\n",
    "        raise ValueError(\"TRAIN_IMAGES is empty\")\n",
    "\n",
    "#     filepath = [\n",
    "#         os.path.join(TRAIN_IMAGES, i)\n",
    "#         for i in os.listdir(TRAIN_IMAGES)[:10]\n",
    "#     ]\n",
    "    \n",
    "    filepath = [\n",
    "        os.path.join(TRAIN_IMAGES, \"00087a6bd4dc_01.jpg\")\n",
    "    ]\n",
    "\n",
    "    for f in filepath:\n",
    "        if not tf.gfile.Exists(f):\n",
    "            raise ValueError(\"file does not exists:\", f)\n",
    "\n",
    "    file_queue = tf.train.string_input_producer(filepath)\n",
    "\n",
    "    image_reader = tf.WholeFileReader()\n",
    "\n",
    "    _, image_file = image_reader.read(file_queue)\n",
    "\n",
    "    image = tf.image.decode_image(image_file)\n",
    "    \n",
    "    image.set_shape((SOURCE_IMAGE_HEIGHT, SOURCE_IMAGE_WIDTH, 3))\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_inputs():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labels():\n",
    "    if not TRAIN_IMAGES:\n",
    "        raise ValueError(\"TRAIN_IMAGES is empty\")\n",
    "        \n",
    "    if not TRAIN_MASKS:\n",
    "        raise ValueError(\"TRAIN_MASKS is empty\")\n",
    "\n",
    "    filepath = [\n",
    "        os.path.join(\n",
    "            TRAIN_MASKS, \n",
    "            os.path.splitext(i)[0]+\"_mask.gif\",\n",
    "        )\n",
    "        for i in [\"00087a6bd4dc_01.jpg\"]\n",
    "    ]\n",
    "\n",
    "    for f in filepath:\n",
    "        if not tf.gfile.Exists(f):\n",
    "            raise ValueError(\"file does not exists:\", f)\n",
    "\n",
    "    file_queue = tf.train.string_input_producer(filepath)\n",
    "\n",
    "    image_reader = tf.WholeFileReader()\n",
    "\n",
    "    _, image_file = image_reader.read(file_queue)\n",
    "\n",
    "    image = tf.image.decode_image(image_file)\n",
    "    \n",
    "    image = tf.gather(image, 0)\n",
    "    \n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    \n",
    "    image.set_shape((SOURCE_IMAGE_HEIGHT, SOURCE_IMAGE_WIDTH, 1))\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_labels():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_pad(\n",
    "    inputs, \n",
    "    resize=True, \n",
    "    resize_shape=(224, 224), \n",
    "    crop_pad=False, \n",
    "    crop_pad_shape=(224, 224),\n",
    "#     random_flip=False,\n",
    "#     random_adjustment=False,\n",
    "#     standardize=True,\n",
    "):\n",
    "    \n",
    "    image = inputs\n",
    "    \n",
    "    if resize:\n",
    "        image = tf.image.resize_images(\n",
    "            inputs, \n",
    "            resize_shape, \n",
    "#             tf.image.ResizeMethod.BICUBIC,\n",
    "            align_corners=True,\n",
    "        )\n",
    "        \n",
    "    if crop_pad:\n",
    "        image = tf.image.resize_image_with_crop_or_pad(\n",
    "            image, \n",
    "            crop_pad_shape[0], \n",
    "            crop_pad_shape[1])\n",
    "        \n",
    "#     if random_flip:\n",
    "#         image = random_flip(image)\n",
    "        \n",
    "#     if random_adjustment:\n",
    "#         image = random_adjustment(image)\n",
    "        \n",
    "#     if standardize:\n",
    "#         image = tf.image.per_image_standardization(image)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_adjustment(\n",
    "    inputs,\n",
    "    brightness=True,\n",
    "    brightness_delta=0.75,\n",
    "    contrast=True,\n",
    "    contrast_lower=0.25,\n",
    "    contrast_upper=1.5,\n",
    "    hue=False,\n",
    "    hue_delta=0.5,\n",
    "    saturation=True,\n",
    "    saturation_lower=0.0,\n",
    "    saturation_upper=1.25,\n",
    "):\n",
    "    \n",
    "    image = inputs\n",
    "    \n",
    "    if brightness:\n",
    "        image = tf.image.random_brightness(image, max_delta=brightness_delta)\n",
    "    \n",
    "    if contrast:\n",
    "        image = tf.image.random_contrast(image, lower=contrast_lower, upper=contrast_upper)\n",
    "    \n",
    "    if hue:\n",
    "        image = tf.image.random_hue(image, hue_delta)\n",
    "    \n",
    "    if saturation:\n",
    "        image = tf.image.random_saturation(image, lower=saturation_lower, upper=saturation_upper)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_flip(\n",
    "    inputs,\n",
    "    vertical=False,\n",
    "    horizontal=True,\n",
    "):\n",
    "    \n",
    "    image = inputs\n",
    "    \n",
    "    if vertical:\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        \n",
    "    if horizontal:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(inputs):\n",
    "    \n",
    "    image = inputs\n",
    "    \n",
    "#     image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "#     image_max = tf.reduce_max(tf.reshape(image, [-1]))\n",
    "    \n",
    "#     image = image / image_max\n",
    "    \n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879386 40 881253 141 883140 205 885009 17 885032 259 886923 308 888839 328 890754 340 892670 347 894587 352 896503 357 898420 360 900336 364 902253 367 904170 370 906086 374 908003 377 909920 379 911837 381 913754 384 915671 386 917588 388 919505 391 921422 393 923339 395 925256 397 927173 399 929090 401 931007 403 932925 404 934842 406 936759 408 938676 410 940593 412 942510 414 944428 415 946345 416 948262 418 950179 420 952097 421 954014 423 955931 425 957848 426 959766 427 961683 429 963600 431 965517 432 967435 433 969352 435 971269 437 973186 438 975104 439 977021 441 978938 442 980855 444 982773 445 984690 447 986607 448 988525 449 990442 451 992359 452 994277 453 996194 455 998111 456 1000028 458 1001946 459 1003863 460 1005780 462 1007698 463 1009615 464 1011532 466 1013450 467 1015367 468 1017284 470 1019202 471 1021119 472 1023036 474 1024954 475 1026871 476 1028788 478 1030706 479 1032623 480 1034540 482 1036458 482 1038375 484 1040292 486 1042210 486 1044127 488 1046044 490 1047962 490 1049879 492 1051796 493 1053714 494 1055631 496 1057549 496 1059466 498 1061383 499 1063301 500 1065218 501 1067135 503 1069053 504 1070970 505 1072888 506 1074805 507 1076722 509 1078640 510 1080557 511 1082475 512 1084392 513 1086309 515 1088227 516 1090144 517 1092062 518 1093979 519 1095897 520 1097814 521 1099731 523 1101649 524 1103566 525 1105484 526 1107401 527 1109319 528 1111236 529 1113153 531 1115071 532 1116988 533 1118906 534 1120823 535 1122741 536 1124658 537 1126576 538 1128493 539 1130410 541 1132328 542 1134245 543 1136163 544 1138080 545 1139920 31 1139998 546 1141835 47 1141915 547 1143751 57 1143833 548 1145667 61 1145750 550 1147584 63 1147668 550 1149501 65 1149585 552 1151418 66 1151503 552 1152090 30 1153336 66 1153420 554 1154004 42 1155253 67 1155337 555 1155920 51 1157171 67 1157255 556 1157837 57 1159088 68 1159172 557 1159754 61 1161006 68 1161090 558 1161672 64 1162923 69 1163007 560 1163589 66 1164841 69 1164925 560 1165507 68 1166759 69 1166842 562 1167424 70 1168676 70 1168760 562 1169342 70 1170594 70 1170677 564 1171260 71 1172512 70 1172595 564 1173178 71 1174430 70 1174512 566 1175096 72 1176347 71 1176430 567 1177014 72 1178265 70 1178347 568 1178932 72 1180183 70 1180265 569 1180850 72 1182101 70 1182182 570 1182767 73 1184019 70 1184100 571 1184685 74 1185937 70 1186017 572 1186603 74 1187854 71 1187935 573 1188521 74 1189772 71 1189852 574 1190439 74 1191690 71 1191770 575 1192357 74 1193608 71 1193687 577 1194275 74 1195526 71 1195605 577 1196193 74 1197444 71 1197522 579 1198111 74 1199362 71 1199440 579 1200029 74 1201280 658 1201947 74 1203198 741 1205116 741 1207035 739 1208953 739 1210872 738 1212791 737 1214710 736 1216630 733 1218551 730 1220472 727 1222392 724 1224314 720 1226235 716 1228156 712 1230079 706 1232002 699 1233926 691 1235851 680 1237776 669 1239701 654 1241623 640 1243540 631 1245458 618 1247376 604 1249293 605 1251211 605 1253128 606 1255046 607 1256963 608 1258881 608 1260798 610 1262715 611 1264633 612 1266550 613 1268467 615 1270385 615 1272302 617 1274219 619 1276136 620 1278053 622 1279970 624 1281887 626 1283804 627 1285721 629 1287637 632 1289554 634 1291471 636 1293388 638 1295304 641 1297221 643 1299138 645 1301054 648 1302971 651 1304887 654 1306804 656 1308721 659 1310637 663 1312554 666 1314471 669 1316387 673 1318304 676 1320221 679 1322138 682 1324055 685 1325972 687 1327889 690 1329806 692 1331723 695 1333640 697 1335558 698 1337475 700 1339392 702 1341310 703 1343227 705 1345144 707 1347062 707 1348979 709 1350896 711 1352814 712 1354731 714 1356649 714 1358566 716 1360484 717 1362401 718 1364319 719 1366237 719 1368154 721 1370072 722 1371989 723 1373907 723 1375824 725 1377742 725 1379660 726 1381577 727 1383495 728 1385412 729 1387330 730 1389247 731 1391165 732 1393082 733 1395000 734 1396918 734 1398835 736 1400753 736 1402670 738 1404588 738 1406505 740 1408423 740 1410340 742 1412258 742 1414175 743 1416093 744 1418010 745 1419928 746 1421845 747 1423763 748 1425680 749 1427598 750 1429515 751 1431433 752 1433350 753 1435268 754 1437185 755 1439103 755 1441020 757 1442937 758 1444855 759 1446772 760 1448690 761 1450607 762 1452525 763 1454442 764 1456360 764 1458278 765 1460195 766 1462113 767 1464030 768 1465948 769 1467866 769 1469783 771 1471701 771 1473619 771 1475536 773 1477454 773 1479372 774 1481289 775 1483207 776 1485125 776 1487042 777 1488960 778 1490878 778 1492796 778 1494713 780 1496631 780 1498549 781 1500467 781 1502384 782 1504302 783 1506220 783 1508138 783 1510056 784 1511973 785 1513891 785 1515809 785 1517727 786 1519645 786 1521563 786 1523480 787 1525398 788 1527316 788 1529234 788 1531152 788 1533070 789 1534987 790 1536905 790 1538823 790 1540741 790 1542659 791 1544577 791 1546495 791 1548412 792 1550330 792 1552248 792 1554166 792 1556084 793 1558002 793 1559919 794 1561837 794 1563755 794 1565673 794 1567591 794 1569509 795 1571427 795 1573345 795 1575262 796 1577180 796 1579098 796 1581016 796 1582934 796 1584852 796 1586770 796 1588687 798 1590605 798 1592523 798 1594441 798 1596359 798 1598277 798 1600195 798 1602113 798 1604030 799 1605948 799 1607866 799 1609784 799 1611702 799 1613620 799 1615538 800 1617456 800 1619374 800 1621291 801 1623209 801 1625127 801 1627045 801 1628963 801 1630881 801 1632799 801 1634717 801 1636635 801 1638553 801 1640470 802 1642388 802 1644306 802 1646224 803 1648142 803 1650060 803 1651978 803 1653896 803 1655814 803 1657732 803 1659650 803 1661568 803 1663486 803 1665404 803 1667322 803 1669239 804 1671157 804 1673075 804 1674993 804 1676911 804 1678829 804 1680747 804 1682665 804 1684583 805 1686501 805 1688419 805 1690337 805 1692255 805 1694173 805 1696091 805 1698009 805 1699927 805 1701845 805 1703763 805 1705681 805 1707599 805 1709517 805 1711435 805 1713353 805 1715271 805 1717189 805 1719107 805 1721025 805 1722943 805 1724861 805 1726779 805 1728697 805 1730615 805 1732533 805 1734451 805 1736369 805 1738287 805 1740205 805 1742123 805 1744041 805 1745959 805 1747877 805 1749795 805 1751713 805 1753631 805 1755549 805 1757467 805 1759385 805 1761303 805 1763221 805 1765140 804 1767058 804 1768976 804 1770894 804 1772812 804 1774730 804 1776648 804 1778566 804 1780484 804 1782402 804 1784320 804 1786238 804 1788156 803 1790074 803 1791992 803 1793911 802 1795829 802 1797747 802 1799665 802 1801583 802 1803501 802 1805419 802 1807337 802 1809255 802 1811173 802 1813091 802 1815010 801 1816928 801 1818846 801 1820764 801 1822682 801 1824600 801 1826518 801 1828436 801 1830354 801 1832272 801 1834190 801 1836108 800 1838026 800 1839943 801 1841861 801 1843779 801 1845697 801 1847615 801 1849533 801 1851451 801 1853369 801 1855287 801 1857205 801 1859123 801 1861041 801 1862959 801 1864877 801 1866795 801 1868713 800 1870631 800 1872549 800 1874467 800 1876385 800 1878303 800 1880221 800 1882139 800 1884057 800 1885975 800 1887893 800 1889811 800 1891730 799 1893648 798 1895566 798 1897484 798 1899402 798 1901320 798 1903238 798 1905157 797 1907075 797 1908993 797 1910911 796 1912829 796 1914748 795 1916666 795 1918584 795 1920502 795 1922421 794 1924339 794 1926257 793 1928176 792 1930094 792 1932012 792 1933931 791 1935849 791 1937767 790 1939686 789 1941604 789 1943522 789 1945440 789 1947359 787 1949277 787 1951195 787 1953114 786 1955032 786 1956950 785 1958868 785 1960787 784 1962705 784 1964623 783 1966541 783 1968460 782 1970378 781 1972296 781 1974214 781 1976133 780 1978051 779 1979969 779 1981887 779 1983806 777 1985724 777 1987642 777 1989561 775 1991479 775 1993397 775 1995315 774 1997234 773 1999152 773 2001070 773 2002989 771 2004907 771 2006825 771 2008744 769 2010662 769 2012581 768 2014499 767 2016418 766 2018336 765 2020255 764 2022174 763 2024092 762 2026011 761 2027930 759 2029849 758 2031768 756 2033687 755 2035606 753 2037525 751 2039444 749 2041364 747 2043283 745 2045203 742 2047123 739 2049043 736 2050964 731 2052885 727 2054806 722 2056729 716 2058651 710 2060575 702 2062499 693 2064424 684 2066349 674 2068276 661 2070203 648 2072131 633 2074060 618 2075992 599 2077924 580 2079857 559 2081793 535 2083729 510 2085669 479 2087611 444 2089562 397 2091525 329 2093510 219 2095486 87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv(TRAIN_MASKS_CSV, header=0)\n",
    "\n",
    "target = csv.loc[0].rle_mask\n",
    "\n",
    "print(csv.loc[0].rle_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    \n",
    "    pixels = img.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    return \" \".join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def rle_encode(mask_image):\n",
    "#     pixels = mask_image.flatten()\n",
    "#     # We avoid issues with '1' at the start or end (at the corners of \n",
    "#     # the original image) by setting those pixels to '0' explicitly.\n",
    "#     # We do not expect these to be non-zero for an accurate mask, \n",
    "#     # so this should not harm the score.\n",
    "#     pixels[0] = 0\n",
    "#     pixels[-1] = 0\n",
    "#     runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "#     runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "#     return runs\n",
    "\n",
    "\n",
    "# def rle_to_string(runs):\n",
    "#     return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_inputs()\n",
    "label = read_labels()\n",
    "\n",
    "# label_max = tf.reduce_max(label)\n",
    "\n",
    "# new_width = FLAGS.TARGET_IMAGE_SIZE\n",
    "# new_height = int(\n",
    "#     float(FLAGS.SOURCE_IMAGE_HEIGHT) / (\n",
    "#         float(FLAGS.SOURCE_IMAGE_WIDTH) / float(FLAGS.TARGET_IMAGE_SIZE)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# image = random_adjustment(image, hue=True)\n",
    "\n",
    "# image = resize_pad(\n",
    "#     image, \n",
    "#     resize=True, \n",
    "#     resize_shape=(new_height, new_width), \n",
    "#     crop_pad=False, \n",
    "#     crop_pad_shape=(\n",
    "#         FLAGS.TARGET_IMAGE_SIZE, \n",
    "#         FLAGS.TARGET_IMAGE_SIZE,\n",
    "#     ),\n",
    "# #     random_flip=False,\n",
    "# #     random_adjustment=True,\n",
    "# #     standardize=False,\n",
    "# )\n",
    "# image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "# image = tf.image.adjust_gamma(image, gamma=4.0)\n",
    "\n",
    "# image = tf.image.adjust_contrast(image, contrast_factor=4)\n",
    "# label_f = tf.contrib.layers.flatten(label)\n",
    "# label_f = tf.reshape(label, [-1])\n",
    "\n",
    "# image = standardize(image)\n",
    "\n",
    "# image_batch = tf.train.shuffle_batch(\n",
    "#     [image],\n",
    "#     batch_size=3,\n",
    "#     num_threads=2,\n",
    "#     capacity=50,\n",
    "#     min_after_dequeue=1)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "tf.local_variables_initializer().run()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    img = label.eval()\n",
    "    \n",
    "    src = rle_encode(img)\n",
    "    \n",
    "    if src != target:\n",
    "        print(\"different\")\n",
    "    \n",
    "#     for hi, h in enumerate(img):\n",
    "#         for wi, w in enumerate(h):\n",
    "            \n",
    "#     print(label.eval())\n",
    "#     print(label.eval())\n",
    "#     plt.imshow(label.eval())\n",
    "#     plt.imshow(image.eval().astype(np.uint8))\n",
    "#     plt.show()\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'b']\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string, [None])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "print(x.eval(feed_dict={\n",
    "    x: [\"a\", \"b\"]\n",
    "}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
